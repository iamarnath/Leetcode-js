/*
609. Find Duplicate File in System


Solution - https://leetcode.com/problems/find-duplicate-file-in-system/solutions/5118662/optimised/

Description
Given a list paths of directory info, including the directory path, and all the files with contents in this directory, return all the duplicate files in the file system in terms of their paths. You may return the answer in any order.

A group of duplicate files consists of at least two files that have the same content.

A single directory info string in the input list has the following format:

"root/d1/d2/.../dm f1.txt(f1_content) f2.txt(f2_content) ... fn.txt(fn_content)"
It means there are n files (f1.txt, f2.txt ... fn.txt) with content (f1_content, f2_content ... fn_content) respectively in the directory "root/d1/d2/.../dm". Note that n >= 1 and m >= 0. If m = 0, it means the directory is just the root directory.

The output is a list of groups of duplicate file paths. For each group, it contains all the file paths of the files that have the same content. A file path is a string that has the following format:

"directory_path/file_name.txt"
 

Example 1:

Input: paths = ["root/a 1.txt(abcd) 2.txt(efgh)","root/c 3.txt(abcd)","root/c/d 4.txt(efgh)","root 4.txt(efgh)"]
Output: [["root/a/2.txt","root/c/d/4.txt","root/4.txt"],["root/a/1.txt","root/c/3.txt"]]
Example 2:

Input: paths = ["root/a 1.txt(abcd) 2.txt(efgh)","root/c 3.txt(abcd)","root/c/d 4.txt(efgh)"]
Output: [["root/a/2.txt","root/c/d/4.txt"],["root/a/1.txt","root/c/3.txt"]]
 

Constraints:

1 <= paths.length <= 2 * 104
1 <= paths[i].length <= 3000
1 <= sum(paths[i].length) <= 5 * 105
paths[i] consist of English letters, digits, '/', '.', '(', ')', and ' '.
You may assume no files or directories share the same name in the same directory.
You may assume each given directory info represents a unique directory. A single blank space separates the directory path and file info.
 

Follow up:

Imagine you are given a real file system, how will you search files? DFS or BFS?
If the file content is very large (GB level), how will you modify your solution?
If you can only read the file by 1kb each time, how will you modify your solution?
What is the time complexity of your modified solution? What is the most time-consuming part and memory-consuming part of it? How to optimize?
How to make sure the duplicated files you find are not false positive?
*/

/*

Approach:
Initialization: The function initializes a Map called fileContentMap to store the file content as the key and the list of file paths with that content as the value.
Parsing File Paths: The function then iterates through each file path in the paths array. For each path, it splits the string on the space character to separate the root directory and the individual files.
Extracting File Information: For each file, the function splits the file name and content using the regular expression $$|$$ to extract the file name and content. It then checks if the file content already exists in the fileContentMap. If it does, it appends the current file path to the existing list of paths. If it doesn't, it initializes a new list with the current file path.
Filtering Duplicates: After processing all the file paths, the function filters the fileContentMap to only include the file contents that have more than one associated file path. It then returns this filtered list of file paths.
Time Complexity:
The time complexity of this function is O(n * m), where n is the number of file paths in the paths array and m is the average number of files per path.
The outer loop iterates through the paths array, which takes O(n) time.
For each path, the function splits the string and iterates through the individual files, which takes O(m) time.
The operations performed on the fileContentMap (checking if the content exists, appending to the list, and updating the map) are all constant-time operations.
The final filtering step, which creates a new array from the fileContentMap values, takes O(n * m) time, as it needs to iterate through all the file paths.
Space Complexity:
The space complexity of this function is O(n * m), where n is the number of file paths in the paths array and m is the average number of files per path.
The fileContentMap stores the file content as the key and the list of file paths as the value. The maximum size of the map is equal to the total number of unique file contents, which is bounded by the total number of files, n * m.
The function also creates a new array to store the filtered file paths, which has a maximum size of n * m, as it includes all the duplicate file paths.


Time Complexity:
The time complexity of this function is O(n * m), where n is the number of file paths in the paths array and m is the average number of files per path.
The outer loop iterates through the paths array, which takes O(n) time.
For each path, the function splits the string and iterates through the individual files, which takes O(m) time.
The operations performed on the fileContentMap (checking if the content exists, appending to the list, and updating the map) are all constant-time operations.
The final filtering step, which creates a new array from the fileContentMap values, takes O(n * m) time, as it needs to iterate through all the file paths.


Space Complexity:
The space complexity of this function is O(n * m), where n is the number of file paths in the paths array and m is the average number of files per path.
The fileContentMap stores the file content as the key and the list of file paths as the value. The maximum size of the map is equal to the total number of unique file contents, which is bounded by the total number of files, n * m.
The function also creates a new array to store the filtered file paths, which has a maximum size of n * m, as it includes all the duplicate file paths.

*/
var findDuplicate = function (paths) {
    // Map to store the content of the files and their paths
    const fileContentMap = new Map();
    // Split the path into root directory and files
    for (const path of paths) {
        const [rootDir, ...files] = path.split(' ');
        // Iterate over each file in the current path
        for (const file of files) {
            const [fileName, fcontent] = file.split(/\(|\)/g).filter(Boolean);
            // Get the existing list of file paths with the same content, or initialize an empty one
            const existingPath = fileContentMap.get(fcontent) ?? [];
            // Add the new file path to the list of paths for this content
            existingPath.push(`${rootDir}/${fileName}`);
            // Update the map with the new list of paths for this content
            fileContentMap.set(fcontent, existingPath);
        }
    }
    // Filter out the file contents that have more than one path and return the values of the map
    let filteredFileContents = [...fileContentMap.values()].filter(paths => paths.length > 1);
    return filteredFileContents;
}

let paths = ["root/a 1.txt(abcd) 2.txt(efgh)", "root/c 3.txt(abcd)", "root/c/d 4.txt(efgh)", "root 4.txt(efgh)"];
paths = ["root/a 1.txt(abcd) 2.txt(efgh)", "root/c 3.txt(abcd)", "root/c/d 4.txt(efgh)"]
console.log(findDuplicate(paths))

/*
Imagine you are given a real file system, how will you search files? DFS or BFS?

Ans -
When dealing with a real file system, the choice between Depth-First Search (DFS) and Breadth-First Search (BFS) depends on various factors. DFS is typically used when the structure is deeper than it is wide, making it suitable for scenarios like solving puzzles with one solution or searching deep trees. On the other hand, BFS is more appropriate for structures that are wider than they are deep, making it ideal for tasks like file system traversal, printing hierarchies, or finding the shortest path between two nodes in a graph.
In the context of a real file system, the decision between DFS and BFS can be influenced by the structure of the file system itself. If the file system is expected to have a greater width than depth, BFS might be more efficient for tasks like traversing directories or searching for files. BFS is particularly useful for scenarios where files are expected to be located relatively close to the root, as it searches the closest nodes first.
However, if the file system's structure is such that it is deeper than it is wide, DFS could be a suitable choice. DFS is generally more memory-efficient for balanced trees, as it requires storing an entire path from the root to a leaf rather than all nodes at a given level like BFS. Additionally, if the file system contains branches that are infinitely long, BFS might be a better option due to its ability to handle such scenarios effectively.
In conclusion, when searching files in a real file system, the decision between DFS and BFS should be based on the specific characteristics of the file system, such as its width, depth, and potential memory constraints.


If the file content is very large (GB level), how will you modify your solution?

Ans -
When dealing with very large file content at the GB level, modifications to the solution are necessary to handle the size efficiently. Here are some considerations and modifications that can be made to accommodate large file content:
Streaming and Chunking: Instead of reading the entire file content into memory at once, implement a streaming mechanism to read the file content in smaller, manageable chunks. This approach reduces memory usage and allows processing of large files without loading everything into memory simultaneously.
Hashing: Instead of comparing the entire content of each file directly, consider using hashing algorithms like MD5 or SHA-256 to generate a unique hash for each file's content. This way, you can compare the hashes to identify potential duplicates without needing to compare the entire content of each file.
Parallel Processing: Utilize parallel processing techniques to handle multiple files concurrently. This can improve performance when processing a large number of files with large content sizes.
Optimized Data Structures: Use efficient data structures like Bloom filters or Trie structures to store and compare file content or hashes. These structures can help in quickly identifying potential duplicates without the need to compare every file with every other file.
Disk I/O Optimization: Optimize disk I/O operations by minimizing unnecessary reads and writes. Consider caching frequently accessed data and optimizing file access patterns to reduce latency.
Memory Management: Implement proper memory management techniques to handle large file content. Ensure that memory is released promptly after processing to prevent memory leaks and optimize resource usage.
By incorporating these modifications and considerations, the solution can be adapted to efficiently handle very large file content at the GB level while still identifying duplicate files based on their content.



Here's how the modified code works:
Instead of storing the file content directly, the code now stores the hashes of the file content in the fileHashMap.
The processPath function is responsible for processing each path and calculating the hash of the file content using the calculateFileHash function.
The calculateFileHash function implements a hashing algorithm (in this example, a simple hash function is used for demonstration purposes) to generate a unique hash for the file content.
The code uses Promise.all to process the paths in parallel, improving the overall performance.
After processing all the paths, the code filters out the file hashes that have more than one path and returns the corresponding file paths.
By using hashing and parallel processing, this modified solution can efficiently handle file content at the GB level without loading the entire content into memory at once. The hashing approach allows for quick comparisons of file content, and the parallel processing helps to distribute the workload across multiple threads or processes.
Please note that in a real-world scenario, you would want to use a more robust hashing algorithm, such as MD5 or SHA-256, to ensure the uniqueness of the file hashes.
*/

var findDuplicateCheckLargeFile = function (paths) {
    // Map to store the hashes of the files and their paths
    const fileHashMap = new Map();
    // Parallel processing function
    const processPath = (path) => {
        const [rootDir, ...files] = path.split(' ');
        // Iterate over each file in the current path
        for (const file of files) {
            const [fileName, fcontent] = file.split(/\(|\)/g).filter(Boolean);
            // Calculate the hash of the file content
            const fileHash = calculateFileHash(fcontent);
            // Get the existing list of file paths with the same hash, or initialize an empty one
            const existingPath = fileHashMap.get(fileHash) ?? [];
            // Add the new file path to the list of paths for this hash
            existingPath.push(`${rootDir}/${fileName}`);
            // Update the map with the new list of paths for this hash
            fileHashMap.set(fileHash, existingPath);
        }
    };

    // Function to calculate the hash of the file content
    const calculateFileHash = (fcontent) => {
        // Implement a hashing algorithm like MD5 or SHA-256 to hash the file content
        // This example uses a simple hash function for demonstration purposes
        let hash = 0;
        for (let i = 0; i < fcontent.length; i++) {
            hash = (hash << 5) - hash + fcontent.charCodeAt(i);
            hash |= 0; // Convert to 32bit integer
        }
        return hash.toString();
    };

    // Process the paths in parallel
    const promises = paths.map(processPath);
    Promise.all(promises).then(() => {
        // Filter out the file hashes that have more than one path and return the values of the map
        let filteredFileHashes = [...fileHashMap.values()].filter(paths => paths.length > 1);
        return filteredFileHashes;
    });
};


/*
If you can only read the file by 1kb each time, how will you modify your solution?

In this modified code:
The calculateFileHash function now reads the file content in 1KB chunks and calculates the hash for each chunk.
The processPath function processes each path asynchronously, calculating the hash for each file content in 1KB chunks.
The code uses Promise.all to process the paths in parallel, ensuring efficient handling of large file content in 1KB increments.
After processing all paths, the code filters out the file hashes with more than one path and returns the corresponding file paths.
By reading the file content in 1KB chunks and calculating the hash incrementally, this modified solution can effectively handle large file content while adhering to the constraint of reading files in smaller segments.
*/
var findDuplicateReadby1KB = function (paths) {
    // Map to store the hashes of the files and their paths
    const fileHashMap = new Map();

    // Function to process each path
    const processPath = async (path) => {
        const [rootDir, ...files] = path.split(' ');
        // Iterate over each file in the current path
        for (const file of files) {
            const [fileName, fcontent] = file.split(/\(|\)/g).filter(Boolean);
            // Calculate the hash of the file content
            const fileHash = await calculateFileHash(fcontent);
            // Get the existing list of file paths with the same hash, or initialize an empty one
            const existingPath = fileHashMap.get(fileHash) ?? [];
            // Add the new file path to the list of paths for this hash
            existingPath.push(`${rootDir}/${fileName}`);
            // Update the map with the new list of paths for this hash
            fileHashMap.set(fileHash, existingPath);
        }
    };

    // Function to calculate the hash of the file content in 1KB chunks
    const calculateFileHash = async (fcontent) => {
        // Implement a hashing algorithm like MD5 or SHA-256 to hash the file content
        // This example uses a simple hash function for demonstration purposes
        let hash = 0;
        for (let i = 0; i < fcontent.length; i += 1024) {
            const chunk = fcontent.substring(i, i + 1024);
            for (let j = 0; j < chunk.length; j++) {
                hash = (hash << 5) - hash + chunk.charCodeAt(j);
                hash |= 0; // Convert to 32bit integer
            }
        }
        return hash.toString();
    };

    // Process the paths in parallel
    const promises = paths.map(processPath);
    Promise.all(promises).then(() => {
        // Filter out the file hashes that have more than one path and return the values of the map
        let filteredFileHashes = [...fileHashMap.values()].filter(paths => paths.length > 1);
        return filteredFileHashes;
    });
};
/*
What is the time complexity of your modified solution? What is the most time-consuming part and memory-consuming part of it? How to optimize?

Ans -
The time complexity of the modified solution for finding duplicate files based on their content, which reads the file content in 1KB chunks, can be analyzed as follows:
Reading the file content in 1KB chunks and calculating the hash for each chunk involves iterating over the content in chunks. If the file size is 
ð‘
N bytes, the number of iterations required would be approximately 
ð‘
/
1024
N/1024, resulting in a time complexity of 
ð‘‚
(
ð‘
)
O(N) for hashing the file content.
Processing each path involves iterating over the files in the path and calculating the hash for each file content. If there are 
ð‘€
M files in a path, the time complexity for processing a single path would be 
ð‘‚
(
ð‘€
)
O(M).
Processing all paths in parallel using Promise.all results in a time complexity of 
ð‘‚
(
ð‘ƒ
)
O(P), where 
ð‘ƒ
P is the number of paths.
Therefore, the overall time complexity of the modified solution can be approximated as 
ð‘‚
(
ð‘
+
ð‘€
+
ð‘ƒ
)
O(N+M+P), where 
ð‘
N is the total size of all file contents, 
ð‘€
M is the total number of files, and 
ð‘ƒ
P is the number of paths.
The most time-consuming part of the solution is likely the hashing process, especially when dealing with large file contents, as it involves iterating over the content in 1KB chunks and calculating the hash for each chunk.
In terms of memory consumption, the most memory-consuming part would be storing the file content and hashes in memory, especially for large files. Additionally, maintaining the map of file hashes and paths can consume memory, especially when dealing with a large number of files and paths.
To optimize the solution:
Efficient Hashing Algorithms: Implementing more efficient hashing algorithms like MD5 or SHA-256 can improve the hashing process's speed and reduce the time complexity.
Streaming and Chunking: Continue reading and processing the file content in smaller chunks to minimize memory usage and optimize processing large files.
Memory Management: Implement proper memory management techniques to release memory promptly after processing to prevent memory leaks and optimize resource usage.
Parallel Processing: Utilize parallel processing techniques effectively to handle multiple files concurrently and improve overall performance.
Optimized Data Structures: Consider using more efficient data structures or algorithms to store and compare file content or hashes, optimizing memory usage and processing speed.
By implementing these optimizations, the solution can be enhanced to handle large file content more efficiently while reducing time complexity and memory consumption.
*/

/*
How to make sure the duplicated files you find are not false positive?

Based on the search results provided, here are some key points on how to ensure the identified duplicate files are not false positives:
Utilize Robust Hashing Algorithms: The search results suggest using more efficient hashing algorithms like MD5 or SHA-256 to calculate the hash of the file content. This helps ensure that the hashes accurately represent the file content and reduces the chances of false positives.
Perform Byte-Level Comparison: The search results mention that some tools like Ashisoft Duplicate File Finder perform a byte-level comparison of the files to identify exact duplicates, rather than just relying on file names or metadata. This approach is more reliable in detecting true duplicates.
Analyze False Positives: The NCBI article 
 discusses analyzing the false positives identified by various de-duplication methods. It suggests looking at factors like publication type, year, and language to understand the nature of the false positives and improve the de-duplication process.
Leverage Multiple Comparison Modes: The search results indicate that some tools, like Wise Duplicate Finder, offer different comparison modes (e.g., exact match, similar files) to provide more accurate results and reduce false positives.
Exclude Specific Files or Folders: The search results mention the ability to exclude certain files or folders from the de-duplication process, which can help avoid false positives caused by intentionally duplicated files or directories.
Verify Duplicate Files: When necessary, the NCBI article 
 suggests retrieving the full-text publication of the identified false-positive duplicate references to determine the actual publication type and language, ensuring the accuracy of the de-duplication process.
By incorporating these strategies, such as using robust hashing algorithms, performing byte-level comparisons, analyzing false positives, and leveraging multiple comparison modes, you can improve the accuracy of the duplicate file detection process and minimize the chances of encountering false positives.


In this modified code:
The calculateFileHash function now uses the SHA-256 hashing algorithm for more robust hashing of the file content.
The verifyDuplicates function checks for true duplicates by performing byte-level comparison of the file contents and ensuring that only unique content is considered.
The getFileContent function is a placeholder for a function that reads the file content based on the file path in a byte-level comparison manner.
By incorporating these modifications, the JavaScript code now includes mechanisms for robust hashing, byte-level comparison, and verification of duplicate files to reduce the chances of false positives and ensure the accuracy of the duplicate file detection process.

*/


const crypto = require('crypto');

var findDuplicateFalsePositive = function (paths) {
    const fileHashMap = new Map();

    const processPath = async (path) => {
        const [rootDir, ...files] = path.split(' ');
        for (const file of files) {
            const [fileName, fcontent] = file.split(/\(|\)/g).filter(Boolean);
            const fileHash = await calculateFileHash(fcontent);
            const existingPaths = fileHashMap.get(fileHash) ?? [];
            existingPaths.push(`${rootDir}/${fileName}`);
            fileHashMap.set(fileHash, existingPaths);
        }
    };

    const calculateFileHash = async (fcontent) => {
        const hash = crypto.createHash('sha256');
        hash.update(fcontent);
        return hash.digest('hex');
    };

    const promises = paths.map(processPath);
    Promise.all(promises).then(() => {
        const filteredFileHashes = [...fileHashMap.values()].filter(paths => paths.length > 1);
        const verifiedDuplicates = verifyDuplicates(filteredFileHashes);
        return verifiedDuplicates;
    });
};

const verifyDuplicates = (duplicates) => {
    const verifiedDuplicates = [];
    for (const paths of duplicates) {
        const uniqueContents = new Set();
        const validPaths = [];
        for (const path of paths) {
            const content = getFileContent(path);
            if (!uniqueContents.has(content)) {
                uniqueContents.add(content);
                validPaths.push(path);
            }
        }
        if (validPaths.length > 1) {
            verifiedDuplicates.push(validPaths);
        }
    }
    return verifiedDuplicates;
};
const fs = require('fs');
const getFileContent = (filePath) => {
    // Implement a function to read the file content based on the file path
    // This function should read the file content in a byte-level comparison manner
    // and return the content as a string
    try {
        const content = fs.readFileSync(filePath, 'utf8');
        return content;
    } catch (error) {
        console.error(`Error reading file: ${filePath}`, error);
        return null;
    }
};

// Usage example
const paths5 = ["root/a 1.txt(abcd) 2.txt(efgh)","root/c 3.txt(abcd)","root/c/d 4.txt(efgh)","root 4.txt(efgh)"];
findDuplicateFalsePositive(paths5);